# intent-shift
Off-policy evaluation in contextual bandits, which evaluates the reward of a target policy given the history of a logged pol- icy, is a task of importance as it provides an estimate of the performance of a new policy without experimenting with it. Existing off-policy evaluation methods in contextual bandits make an oversimplified assumption that the distribution of contexts is stationary. In this paper, we consider a more prac- tical setting of a context/reward distribution shift between the logged data and the contexts observed for evaluating a target policy in the future. We propose an intent shift model which introduces a latent intent variable to capture the distribution shift on context and reward, avoiding the intractable prob- lem of density estimation of contexts in high-dimension. Un- der the intent shift model, we introduce a consistent spectral- based IPS estimator, characterize its finite-sample complexity and derive an MSE bound on the performance of the final re- ward estimation. Experiments demonstrate that the proposed spectral-based IPS estimator outperforms the existing estimators under distribution shift.
